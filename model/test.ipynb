{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from utils import Data, plot_losses, calculate_probability, flip_from_probability, save_data, load_data\n",
    "from model import StockGPT2Model\n",
    "from torch.utils.data import DataLoader\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device: cuda\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of GPT2Model were not initialized from the model checkpoint at gpt2 and are newly initialized because the shapes did not match:\n",
      "- h.0.attn.c_attn.bias: found shape torch.Size([2304]) in the checkpoint and torch.Size([756]) in the model instantiated\n",
      "- h.0.attn.c_attn.weight: found shape torch.Size([768, 2304]) in the checkpoint and torch.Size([252, 756]) in the model instantiated\n",
      "- h.0.attn.c_proj.bias: found shape torch.Size([768]) in the checkpoint and torch.Size([252]) in the model instantiated\n",
      "- h.0.attn.c_proj.weight: found shape torch.Size([768, 768]) in the checkpoint and torch.Size([252, 252]) in the model instantiated\n",
      "- h.0.ln_1.bias: found shape torch.Size([768]) in the checkpoint and torch.Size([252]) in the model instantiated\n",
      "- h.0.ln_1.weight: found shape torch.Size([768]) in the checkpoint and torch.Size([252]) in the model instantiated\n",
      "- h.0.ln_2.bias: found shape torch.Size([768]) in the checkpoint and torch.Size([252]) in the model instantiated\n",
      "- h.0.ln_2.weight: found shape torch.Size([768]) in the checkpoint and torch.Size([252]) in the model instantiated\n",
      "- h.0.mlp.c_fc.bias: found shape torch.Size([3072]) in the checkpoint and torch.Size([504]) in the model instantiated\n",
      "- h.0.mlp.c_fc.weight: found shape torch.Size([768, 3072]) in the checkpoint and torch.Size([252, 504]) in the model instantiated\n",
      "- h.0.mlp.c_proj.bias: found shape torch.Size([768]) in the checkpoint and torch.Size([252]) in the model instantiated\n",
      "- h.0.mlp.c_proj.weight: found shape torch.Size([3072, 768]) in the checkpoint and torch.Size([504, 252]) in the model instantiated\n",
      "- h.1.attn.c_attn.bias: found shape torch.Size([2304]) in the checkpoint and torch.Size([756]) in the model instantiated\n",
      "- h.1.attn.c_attn.weight: found shape torch.Size([768, 2304]) in the checkpoint and torch.Size([252, 756]) in the model instantiated\n",
      "- h.1.attn.c_proj.bias: found shape torch.Size([768]) in the checkpoint and torch.Size([252]) in the model instantiated\n",
      "- h.1.attn.c_proj.weight: found shape torch.Size([768, 768]) in the checkpoint and torch.Size([252, 252]) in the model instantiated\n",
      "- h.1.ln_1.bias: found shape torch.Size([768]) in the checkpoint and torch.Size([252]) in the model instantiated\n",
      "- h.1.ln_1.weight: found shape torch.Size([768]) in the checkpoint and torch.Size([252]) in the model instantiated\n",
      "- h.1.ln_2.bias: found shape torch.Size([768]) in the checkpoint and torch.Size([252]) in the model instantiated\n",
      "- h.1.ln_2.weight: found shape torch.Size([768]) in the checkpoint and torch.Size([252]) in the model instantiated\n",
      "- h.1.mlp.c_fc.bias: found shape torch.Size([3072]) in the checkpoint and torch.Size([504]) in the model instantiated\n",
      "- h.1.mlp.c_fc.weight: found shape torch.Size([768, 3072]) in the checkpoint and torch.Size([252, 504]) in the model instantiated\n",
      "- h.1.mlp.c_proj.bias: found shape torch.Size([768]) in the checkpoint and torch.Size([252]) in the model instantiated\n",
      "- h.1.mlp.c_proj.weight: found shape torch.Size([3072, 768]) in the checkpoint and torch.Size([504, 252]) in the model instantiated\n",
      "- h.2.attn.c_attn.bias: found shape torch.Size([2304]) in the checkpoint and torch.Size([756]) in the model instantiated\n",
      "- h.2.attn.c_attn.weight: found shape torch.Size([768, 2304]) in the checkpoint and torch.Size([252, 756]) in the model instantiated\n",
      "- h.2.attn.c_proj.bias: found shape torch.Size([768]) in the checkpoint and torch.Size([252]) in the model instantiated\n",
      "- h.2.attn.c_proj.weight: found shape torch.Size([768, 768]) in the checkpoint and torch.Size([252, 252]) in the model instantiated\n",
      "- h.2.ln_1.bias: found shape torch.Size([768]) in the checkpoint and torch.Size([252]) in the model instantiated\n",
      "- h.2.ln_1.weight: found shape torch.Size([768]) in the checkpoint and torch.Size([252]) in the model instantiated\n",
      "- h.2.ln_2.bias: found shape torch.Size([768]) in the checkpoint and torch.Size([252]) in the model instantiated\n",
      "- h.2.ln_2.weight: found shape torch.Size([768]) in the checkpoint and torch.Size([252]) in the model instantiated\n",
      "- h.2.mlp.c_fc.bias: found shape torch.Size([3072]) in the checkpoint and torch.Size([504]) in the model instantiated\n",
      "- h.2.mlp.c_fc.weight: found shape torch.Size([768, 3072]) in the checkpoint and torch.Size([252, 504]) in the model instantiated\n",
      "- h.2.mlp.c_proj.bias: found shape torch.Size([768]) in the checkpoint and torch.Size([252]) in the model instantiated\n",
      "- h.2.mlp.c_proj.weight: found shape torch.Size([3072, 768]) in the checkpoint and torch.Size([504, 252]) in the model instantiated\n",
      "- h.3.attn.c_attn.bias: found shape torch.Size([2304]) in the checkpoint and torch.Size([756]) in the model instantiated\n",
      "- h.3.attn.c_attn.weight: found shape torch.Size([768, 2304]) in the checkpoint and torch.Size([252, 756]) in the model instantiated\n",
      "- h.3.attn.c_proj.bias: found shape torch.Size([768]) in the checkpoint and torch.Size([252]) in the model instantiated\n",
      "- h.3.attn.c_proj.weight: found shape torch.Size([768, 768]) in the checkpoint and torch.Size([252, 252]) in the model instantiated\n",
      "- h.3.ln_1.bias: found shape torch.Size([768]) in the checkpoint and torch.Size([252]) in the model instantiated\n",
      "- h.3.ln_1.weight: found shape torch.Size([768]) in the checkpoint and torch.Size([252]) in the model instantiated\n",
      "- h.3.ln_2.bias: found shape torch.Size([768]) in the checkpoint and torch.Size([252]) in the model instantiated\n",
      "- h.3.ln_2.weight: found shape torch.Size([768]) in the checkpoint and torch.Size([252]) in the model instantiated\n",
      "- h.3.mlp.c_fc.bias: found shape torch.Size([3072]) in the checkpoint and torch.Size([504]) in the model instantiated\n",
      "- h.3.mlp.c_fc.weight: found shape torch.Size([768, 3072]) in the checkpoint and torch.Size([252, 504]) in the model instantiated\n",
      "- h.3.mlp.c_proj.bias: found shape torch.Size([768]) in the checkpoint and torch.Size([252]) in the model instantiated\n",
      "- h.3.mlp.c_proj.weight: found shape torch.Size([3072, 768]) in the checkpoint and torch.Size([504, 252]) in the model instantiated\n",
      "- ln_f.bias: found shape torch.Size([768]) in the checkpoint and torch.Size([252]) in the model instantiated\n",
      "- ln_f.weight: found shape torch.Size([768]) in the checkpoint and torch.Size([252]) in the model instantiated\n",
      "- wpe.weight: found shape torch.Size([1024, 768]) in the checkpoint and torch.Size([1024, 252]) in the model instantiated\n",
      "- wte.weight: found shape torch.Size([50257, 768]) in the checkpoint and torch.Size([50257, 252]) in the model instantiated\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "file_path = \"/home/summer_20/Divyam/StockGPT/model/\"\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print('Device:', device)\n",
    "\n",
    "batch_size = 4\n",
    "# data.completed_tickers.append(\"LDO-USD\")\n",
    "data = load_data(file_path)\n",
    "model = StockGPT2Model(num_features = data.num_features, num_tickers = data.num_tickers).to(device)\n",
    "model.load_state_dict(torch.load(\"/home/summer_20/Divyam/StockGPT/model/best_model_weights.pth\"))\n",
    "data_loader = DataLoader(data, batch_size=batch_size, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.03421285442334223\n"
     ]
    }
   ],
   "source": [
    "criterion = nn.MSELoss()\n",
    "with torch.no_grad():\n",
    "    model.eval()\n",
    "    data.mode = \"val\"\n",
    "    all_outputs = []\n",
    "    all_targets = []\n",
    "    val_loss = 0\n",
    "    for batch_idx, (src, sos) in enumerate(data_loader):\n",
    "        src, sos = src.to(device), sos.to(device)\n",
    "        temp_src = src[:,:data.len_input]\n",
    "        # Forward pass\n",
    "        output, past_key_values = model(src = temp_src, sos = sos)\n",
    "        temp_src = output[:,-1,:].unsqueeze(1)\n",
    "        for i in range(data.len_input, data.len_input + data.len_output - 1):\n",
    "            temp_output, past_key_values = model(src = temp_src, past_key_values=past_key_values)\n",
    "            temp_src = temp_output\n",
    "            output = torch.cat((output,temp_output), dim = 1)\n",
    "        \n",
    "        # output = model(batch_input, batch_target[:,:-1], sos)\n",
    "        # print(output)\n",
    "      # Reshape output and batch_target\n",
    "        output = output.reshape(-1, data.num_features)  # [batch_size * seq_len, 3]\n",
    "        src = src.view(-1, data.num_features)  # [batch_size * seq_len, 3]\n",
    "\n",
    "        all_outputs.append(output.to(device))\n",
    "        all_targets.append(src)\n",
    "        loss = criterion(output.to(device), src)\n",
    "        val_loss += loss.item()\n",
    "    val_loss = val_loss / len(data_loader)\n",
    "\n",
    "# Concatenate all batches\n",
    "all_outputs = torch.cat(all_outputs, dim=0)\n",
    "all_targets = torch.cat(all_targets, dim=0)\n",
    "print(val_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(8, 6))\n",
    "for index in range(100):\n",
    "    fig.clear()\n",
    "    plt.plot(all_targets[index*800:(index+1)*800, 0].to(\"cpu\"), label=\"Target\", color=\"orange\")\n",
    "    plt.plot(all_outputs[index*800:(index+1)*800, 0].to(\"cpu\"), label=\"Output\", color=\"blue\")\n",
    "\n",
    "    plt.xlabel(\"Time Step\")\n",
    "    plt.ylabel(f\"Feature {0+1}\")\n",
    "    plt.title(f\"Feature {0+1} - Target vs. Output\")\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    plt.waitforbuttonpress()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "R2 Scores for each output dimension: [0.70136543 0.68302082 0.68703538 0.69523595 0.51779762]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import r2_score\n",
    "\n",
    "# Calculate R2 score for each output dimension\n",
    "r2_scores = r2_score(all_targets.cpu(), all_outputs.cpu(), multioutput='raw_values')\n",
    "\n",
    "print(\"R2 Scores for each output dimension:\", r2_scores)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10 (default, May 26 2023, 14:05:08) \n[GCC 9.4.0]"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "916dbcbb3f70747c44a77c7bcd40155683ae19c65e1c03b4aa3499c5328201f1"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
